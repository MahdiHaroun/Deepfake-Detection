{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746124847221
        }
      },
      "outputs": [],
      "source": [
        "'''import os\n",
        "import zipfile\n",
        "local_zip = 'real-vs-fake.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('')\n",
        "zip_ref.close()'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1746124895813
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from scikit-learn) (2.1.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-01 18:41:19.497994: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746124880.472994    3603 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746124880.763810    3603 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1746124883.431029    3603 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1746124883.431052    3603 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1746124883.431055    3603 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1746124883.431057    3603 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-05-01 18:41:23.711354: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "%pip install scikit-learn --upgrade\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as pl \n",
        "import os \n",
        "from tensorflow.keras.models import Sequential \n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping , ModelCheckpoint\n",
        "from sklearn.metrics import classification_report, confusion_matrix \n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import layers, models, regularizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1746124899514
        }
      },
      "outputs": [],
      "source": [
        "img_rows, img_cols = 224 , 224\n",
        "batch_size = 32 \n",
        "SEED = 42 \n",
        "num_classes = 2 \n",
        "train_data_dir = 'real-vs-fake/train'\n",
        "validation_data_dir = 'real-vs-fake/valida'\n",
        "test_data_dir = 'real-vs-fake/test'\n",
        "labels = ['Real', 'Fake']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746124920689
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 100000 files belonging to 2 classes.\n",
            "Found 20000 files belonging to 2 classes.\n",
            "Found 20000 files belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1746124913.401245    3603 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79088 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0001:00:00.0, compute capability: 8.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_data_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    image_size=(img_rows, img_cols),\n",
        "    batch_size=batch_size,\n",
        "    seed=SEED,\n",
        "    shuffle=True\n",
        ").map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y)).prefetch(tf.data.AUTOTUNE) #allows TensorFlow to load data in the background while the model is training or predicting, improving performance\n",
        "\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    validation_data_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    image_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    seed=SEED,\n",
        "    shuffle=True\n",
        ").map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y)).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    test_data_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    image_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ").map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y)).prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training with InceptionResNetV2 by freezing the base model’s layers\n",
        "#### 512 Dence layers , LeakyRelu , L2 Regularization , 0.5 dropout \n",
        "#### Early Stop , Adam Optimzer with exp decay lr schdeular \n",
        "#### 10 epochs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1746115672977
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m219055592/219055592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "base_model = InceptionResNetV2(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=(img_rows, img_cols, 3),\n",
        "    pooling='avg'\n",
        ")\n",
        "base_model.trainable = False\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746117837973
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from keras import layers, models, regularizers\n",
        "from tensorflow.keras import Model, Input\n",
        "\n",
        "input_tensor = Input(shape=(224, 224, 3))\n",
        "x = base_model.output\n",
        "x = layers.Dense(512, kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "x = layers.LeakyReLU(alpha=0.2)(x) \n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "output = layers.Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=input_tensor, outputs=output)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1746117841947
        }
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "\n",
        "\n",
        "\n",
        "def exp_decay(epoch):\n",
        "    initial_lrate = 1e-5\n",
        "    k = 0.1\n",
        "    return initial_lrate * np.exp(-k * epoch)\n",
        "\n",
        "class LossHistory_(Callback):\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.losses = []\n",
        "        self.lr = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current_lr = exp_decay(epoch)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.lr.append(current_lr)\n",
        "        print(f\"Epoch {epoch+1} - Learning Rate: {current_lr:.6f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1746117847836
        }
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1746117857068
        }
      },
      "outputs": [],
      "source": [
        "loss_history_ = LossHistory_()\n",
        "lrate_ = LearningRateScheduler(exp_decay)\n",
        "\n",
        "keras_callbacks = [\n",
        "    EarlyStopping(monitor='loss', patience=5, mode='min', min_delta=0.01),\n",
        "    ModelCheckpoint(checkpoint_path, monitor='loss', save_best_only=True, mode='min')\n",
        "]\n",
        "\n",
        "callbacks_list = [loss_history_, lrate_] + keras_callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1746116992584
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3123/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6939 - loss: 5.8418Epoch 1 - Learning Rate: 0.000010\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 38ms/step - accuracy: 0.6939 - loss: 5.8415 - val_accuracy: 0.7700 - val_loss: 4.7934 - learning_rate: 1.0000e-05\n",
            "Epoch 2/10\n",
            "\u001b[1m3124/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7248 - loss: 4.6702Epoch 2 - Learning Rate: 0.000009\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 37ms/step - accuracy: 0.7248 - loss: 4.6701 - val_accuracy: 0.7872 - val_loss: 3.9821 - learning_rate: 9.0484e-06\n",
            "Epoch 3/10\n",
            "\u001b[1m3124/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7400 - loss: 3.9238Epoch 3 - Learning Rate: 0.000008\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 34ms/step - accuracy: 0.7400 - loss: 3.9237 - val_accuracy: 0.7963 - val_loss: 3.4266 - learning_rate: 8.1873e-06\n",
            "Epoch 4/10\n",
            "\u001b[1m3124/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7524 - loss: 3.4017Epoch 4 - Learning Rate: 0.000007\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 34ms/step - accuracy: 0.7524 - loss: 3.4017 - val_accuracy: 0.8052 - val_loss: 3.0353 - learning_rate: 7.4082e-06\n",
            "Epoch 5/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7614 - loss: 3.0349Epoch 5 - Learning Rate: 0.000007\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 34ms/step - accuracy: 0.7614 - loss: 3.0349 - val_accuracy: 0.8134 - val_loss: 2.7479 - learning_rate: 6.7032e-06\n",
            "Epoch 6/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7740 - loss: 2.7612Epoch 6 - Learning Rate: 0.000006\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 34ms/step - accuracy: 0.7741 - loss: 2.7612 - val_accuracy: 0.8162 - val_loss: 2.5320 - learning_rate: 6.0653e-06\n",
            "Epoch 7/10\n",
            "\u001b[1m3124/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7812 - loss: 2.5525Epoch 7 - Learning Rate: 0.000005\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 34ms/step - accuracy: 0.7812 - loss: 2.5524 - val_accuracy: 0.8231 - val_loss: 2.3624 - learning_rate: 5.4881e-06\n",
            "Epoch 8/10\n",
            "\u001b[1m3124/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7852 - loss: 2.3888Epoch 8 - Learning Rate: 0.000005\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 34ms/step - accuracy: 0.7852 - loss: 2.3888 - val_accuracy: 0.8251 - val_loss: 2.2292 - learning_rate: 4.9659e-06\n",
            "Epoch 9/10\n",
            "\u001b[1m3124/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7909 - loss: 2.2619Epoch 9 - Learning Rate: 0.000004\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 34ms/step - accuracy: 0.7909 - loss: 2.2619 - val_accuracy: 0.8289 - val_loss: 2.1208 - learning_rate: 4.4933e-06\n",
            "Epoch 10/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7940 - loss: 2.1554Epoch 10 - Learning Rate: 0.000004\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 35ms/step - accuracy: 0.7940 - loss: 2.1554 - val_accuracy: 0.8312 - val_loss: 2.0303 - learning_rate: 4.0657e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x76fbd8283a00>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_ds, validation_data=val_ds, epochs=10, verbose=1, callbacks=callbacks_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Results: \n",
        "#### AVG Time Per Epoch = 109s\n",
        "#### GPU = NVIDIA A100 Tensor Core GPU \n",
        "#### Acc = 0.79"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fine-Tuning with InceptionResNetV2 by freezing the first 450 layers and leaves the rest trainable.\n",
        "#### 512 Dence layers , Relu , L2 Regularization , 0.5 dropout \n",
        "#### Early Stop , Adam Optimzer with 1e-6 Lr  \n",
        "#### 5 epochs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1746117860632
        }
      },
      "outputs": [],
      "source": [
        "for layer in base_model.layers[:450]:\n",
        "        layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1746117865360
        }
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-6),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1746118427126
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3124/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6267 - loss: 7.7850Epoch 1 - Learning Rate: 0.000010\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 37ms/step - accuracy: 0.6267 - loss: 7.7845 - val_accuracy: 0.7509 - val_loss: 5.9306 - learning_rate: 1.0000e-05\n",
            "Epoch 2/5\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6951 - loss: 5.7220Epoch 2 - Learning Rate: 0.000009\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 34ms/step - accuracy: 0.6951 - loss: 5.7219 - val_accuracy: 0.7764 - val_loss: 4.7061 - learning_rate: 9.0484e-06\n",
            "Epoch 3/5\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7186 - loss: 4.6285Epoch 3 - Learning Rate: 0.000008\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 35ms/step - accuracy: 0.7186 - loss: 4.6285 - val_accuracy: 0.7890 - val_loss: 3.9630 - learning_rate: 8.1873e-06\n",
            "Epoch 4/5\n",
            "\u001b[1m3123/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7379 - loss: 3.9381Epoch 4 - Learning Rate: 0.000007\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 34ms/step - accuracy: 0.7379 - loss: 3.9380 - val_accuracy: 0.7977 - val_loss: 3.4600 - learning_rate: 7.4082e-06\n",
            "Epoch 5/5\n",
            "\u001b[1m3124/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7449 - loss: 3.4741Epoch 5 - Learning Rate: 0.000007\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 34ms/step - accuracy: 0.7449 - loss: 3.4740 - val_accuracy: 0.8033 - val_loss: 3.0986 - learning_rate: 6.7032e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x76fb93753460>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_ds, validation_data=val_ds, epochs=5, callbacks=callbacks_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Results: \n",
        "#### AVG Time Per Epoch = 112s\n",
        "#### GPU = NVIDIA A100 Tensor Core GPU \n",
        "#### Acc = 0.744\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine Tuning #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fine-Tuning with InceptionResNetV2 by freezing the first 450 layers and leaves the rest trainable.\n",
        "#### 512 Dence layers , Relu , L2 Regularization , 0.5 dropout \n",
        "#### Early Stop , Adam Optimzer with 0.0001 \n",
        "#### 10 epochs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746124927562
        }
      },
      "outputs": [],
      "source": [
        "base_model = InceptionResNetV2(\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    pooling='avg'\n",
        "      \n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1746124933250
        }
      },
      "outputs": [],
      "source": [
        "for layer in base_model.layers[:450]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746124934646
        }
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import models\n",
        "\n",
        "model = models.Sequential([\n",
        "        base_model,                               ,         \n",
        "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),  \n",
        "        layers.BatchNormalization(),            \n",
        "        layers.Dropout(0.5),                     \n",
        "        layers.Dense(1, activation='sigmoid')    \n",
        "    ])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746124936217
        }
      },
      "outputs": [],
      "source": [
        "model.compile( optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746126662344
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 56ms/step - accuracy: 0.8618 - loss: 4.4749 - val_accuracy: 0.9718 - val_loss: 0.1884\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 54ms/step - accuracy: 0.9730 - loss: 0.1191 - val_accuracy: 0.9822 - val_loss: 0.0623\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 53ms/step - accuracy: 0.9865 - loss: 0.0461 - val_accuracy: 0.9753 - val_loss: 0.0786\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 53ms/step - accuracy: 0.9926 - loss: 0.0260 - val_accuracy: 0.9815 - val_loss: 0.0613\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 5/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 53ms/step - accuracy: 0.9930 - loss: 0.0269 - val_accuracy: 0.9814 - val_loss: 0.0586\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 6/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 52ms/step - accuracy: 0.9954 - loss: 0.0170 - val_accuracy: 0.9722 - val_loss: 0.0910\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 7/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 53ms/step - accuracy: 0.9967 - loss: 0.0141 - val_accuracy: 0.9897 - val_loss: 0.0334\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 8/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 52ms/step - accuracy: 0.9972 - loss: 0.0112 - val_accuracy: 0.9918 - val_loss: 0.0271\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 9/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 52ms/step - accuracy: 0.9978 - loss: 0.0100 - val_accuracy: 0.9704 - val_loss: 0.1207\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 10/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 52ms/step - accuracy: 0.9973 - loss: 0.0105 - val_accuracy: 0.9037 - val_loss: 0.4831\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
          ]
        }
      ],
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss',  patience=4, restore_best_weights=True)\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=10, early_stopping=early_stopping, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746126760575
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "model.save(\"final_inceptionresnetv2_deepfake_model_99.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Results: \n",
        "#### AVG Time Per Epoch = 164s\n",
        "#### GPU = NVIDIA A100 Tensor Core GPU\n",
        "#### ACC = 0.9973 , Loss - 0.4831"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746126695671
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 30ms/step - accuracy: 0.9918 - loss: 0.0254\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Test Loss: 0.0292\n",
            "Test Accuracy: 0.9909\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
        "\n",
        "\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\") \n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - Pytorch and Tensorflow",
      "language": "python",
      "name": "python38-azureml-pt-tf"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
